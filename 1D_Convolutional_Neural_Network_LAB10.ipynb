{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO27wHEj2L/HxuqF+JcSgFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristyAlex2147215/NNDL/blob/main/1D_Convolutional_Neural_Network_LAB10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install patool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhtwPHcBz6DS",
        "outputId": "d0d79ad6-148e-4314-9dc4-f46bb8e01a72"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: patool in /usr/local/lib/python3.8/dist-packages (1.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import patoolib as patool\n",
        "dir(patool)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrCKXJQ20cHI",
        "outputId": "a9be9296-22ae-43f4-caab-2fceaaa0afdc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['App',\n",
              " 'ArchiveCommands',\n",
              " 'ArchiveCompressions',\n",
              " 'ArchiveFormats',\n",
              " 'ArchiveMimetypes',\n",
              " 'ArchivePrograms',\n",
              " 'ProgramModules',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " '_create_archive',\n",
              " '_diff_archives',\n",
              " '_extract_archive',\n",
              " '_handle_archive',\n",
              " '_recompress_archive',\n",
              " '_repack_archive',\n",
              " '_search_archive',\n",
              " 'check_archive_format',\n",
              " 'check_program_compression',\n",
              " 'cleanup_outdir',\n",
              " 'configuration',\n",
              " 'create_archive',\n",
              " 'diff_archives',\n",
              " 'extract_archive',\n",
              " 'find_archive_program',\n",
              " 'get_archive_cmdlist_func',\n",
              " 'get_archive_format',\n",
              " 'importlib',\n",
              " 'list_archive',\n",
              " 'list_formats',\n",
              " 'lzma',\n",
              " 'make_dir_readable',\n",
              " 'make_file_readable',\n",
              " 'make_user_readable',\n",
              " 'move_outdir_orphan',\n",
              " 'os',\n",
              " 'print_function',\n",
              " 'program_supports_compression',\n",
              " 'py_lzma',\n",
              " 'recompress_archive',\n",
              " 'repack_archive',\n",
              " 'rmtree_log_error',\n",
              " 'run_archive_cmdlist',\n",
              " 'search_archive',\n",
              " 'shutil',\n",
              " 'stat',\n",
              " 'sys',\n",
              " 'test_archive',\n",
              " 'util']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patool.extract_archive(\"/content/UCI HAR Dataset.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "r6F7AUPC1GQg",
        "outputId": "eecc7089-9a0c-4f9d-fb9c-00f286959165"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patool: Extracting /content/UCI HAR Dataset.zip ...\n",
            "patool: running /usr/bin/7z x -o./Unpack_bmgh2fbs -- \"/content/UCI HAR Dataset.zip\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PatoolError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPatoolError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e2e135aa4edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpatool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/UCI HAR Dataset.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36mextract_archive\u001b[0;34m(archive, verbosity, outdir, program, interactive)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbosity\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting %s ...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36m_extract_archive\u001b[0;34m(archive, verbosity, interactive, outdir, program, format, compression)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# already handled the command (eg. when it's a builtin Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mrun_archive_cmdlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_cleanup_outdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanup_outdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36mrun_archive_cmdlist\u001b[0;34m(archive_cmdlist, verbosity)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive_cmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_checked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrunkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/patoolib/util.py\u001b[0m in \u001b[0;36mrun_checked\u001b[0;34m(cmd, ret_ok, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Command `%s' returned non-zero exit status %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPatoolError\u001b[0m: Command `['/usr/bin/7z', 'x', '-o./Unpack_bmgh2fbs', '--', '/content/UCI HAR Dataset.zip']' returned non-zero exit status 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip \"UCIHARDataset.zip\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4pBEguRy83B",
        "outputId": "40bff099-07ac-471e-c0fa-072817d73862"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  UCIHARDataset.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of UCIHARDataset.zip or\n",
            "        UCIHARDataset.zip.zip, and cannot find UCIHARDataset.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ihHJaAJuGhP"
      },
      "outputs": [],
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        " dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        " return dataframe.values"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load a list of files into a 3D array of [samples, timesteps, features]\n",
        "def load_group(filenames, prefix=''):\n",
        " loaded = list()\n",
        " for name in filenames:\n",
        " data = load_file(prefix + name)\n",
        " loaded.append(data)\n",
        " # stack group so that features are the 3rd dimension\n",
        " loaded = dstack(loaded)\n",
        " return loaded"
      ],
      "metadata": {
        "id": "A4sMcC-mubOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        " filepath = prefix + group + '/Inertial Signals/'\n",
        " # load all 9 files as a single array\n",
        " filenames = list()\n",
        " # total acceleration\n",
        " filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        " # body acceleration\n",
        " filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        " # body gyroscope\n",
        " filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        " # load input data\n",
        " X = load_group(filenames, filepath)\n",
        " # load class output\n",
        " y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        " return X, y"
      ],
      "metadata": {
        "id": "T7WMIYUSuc5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        " # load all train\n",
        " trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        " print(trainX.shape, trainy.shape)\n",
        " # load all test\n",
        " testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        " print(testX.shape, testy.shape)\n",
        " # zero-offset class values\n",
        " trainy = trainy - 1\n",
        " testy = testy - 1\n",
        " # one hot encode y\n",
        " trainy = to_categorical(trainy)\n",
        " testy = to_categorical(testy)\n",
        " print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        " return trainX, trainy, testX, testy"
      ],
      "metadata": {
        "id": "8FL-4xWxugTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]"
      ],
      "metadata": {
        "id": "r4CgXsiGujvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(n_outputs, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UvaNJ6AjumcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        " verbose, epochs, batch_size = 0, 10, 32\n",
        " n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        " model = Sequential()\n",
        " model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        " model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        " model.add(Dropout(0.5))\n",
        " model.add(MaxPooling1D(pool_size=2))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(100, activation='relu'))\n",
        " model.add(Dense(n_outputs, activation='softmax'))\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " # fit network\n",
        " model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        " # evaluate model\n",
        " _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        " return accuracy"
      ],
      "metadata": {
        "id": "bCvYJELiuo9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repeat experiment\n",
        "scores = list()\n",
        "for r in range(repeats):\n",
        " score = evaluate_model(trainX, trainy, testX, testy)\n",
        " score = score * 100.0\n",
        " print('>#%d: %.3f' % (r+1, score))\n",
        " scores.append(score)"
      ],
      "metadata": {
        "id": "ZkZNqbZjurmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_results(scores):\n",
        " print(scores)\n",
        " m, s = mean(scores), std(scores)\n",
        " print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))"
      ],
      "metadata": {
        "id": "tYodv07FutOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# run an experiment\n",
        "def run_experiment(repeats=10):\n",
        " # load data\n",
        " trainX, trainy, testX, testy = load_dataset()\n",
        " # repeat experiment\n",
        " scores = list()\n",
        " for r in range(repeats):\n",
        " score = evaluate_model(trainX, trainy, testX, testy)\n",
        " score = score * 100.0\n",
        " print('>#%d: %.3f' % (r+1, score))\n",
        " scores.append(score)\n",
        " # summarize results\n",
        " summarize_results(scores)"
      ],
      "metadata": {
        "id": "ZA6hI7z7uva9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# cnn model\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from numpy import dstack\n",
        "from pandas import read_csv\n",
        "from matplotlib import pyplot\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.utils import to_categorical\n",
        " \n",
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        " dataframe = read_csv(filepath, header=None, delim_whitespace=True)\n",
        " return dataframe.values\n",
        " \n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        " loaded = list()\n",
        " for name in filenames:\n",
        " data = load_file(prefix + name)\n",
        " loaded.append(data)\n",
        " # stack group so that features are the 3rd dimension\n",
        " loaded = dstack(loaded)\n",
        " return loaded\n",
        " \n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        " filepath = prefix + group + '/Inertial Signals/'\n",
        " # load all 9 files as a single array\n",
        " filenames = list()\n",
        " # total acceleration\n",
        " filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        " # body acceleration\n",
        " filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        " # body gyroscope\n",
        " filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        " # load input data\n",
        " X = load_group(filenames, filepath)\n",
        " # load class output\n",
        " y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        " return X, y\n",
        " \n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        " # load all train\n",
        " trainX, trainy = load_dataset_group('train', prefix + 'HARDataset/')\n",
        " print(trainX.shape, trainy.shape)\n",
        " # load all test\n",
        " testX, testy = load_dataset_group('test', prefix + 'HARDataset/')\n",
        " print(testX.shape, testy.shape)\n",
        " # zero-offset class values\n",
        " trainy = trainy - 1\n",
        " testy = testy - 1\n",
        " # one hot encode y\n",
        " trainy = to_categorical(trainy)\n",
        " testy = to_categorical(testy)\n",
        " print(trainX.shape, trainy.shape, testX.shape, testy.shape)\n",
        " return trainX, trainy, testX, testy\n",
        " \n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        " verbose, epochs, batch_size = 0, 10, 32\n",
        " n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        " model = Sequential()\n",
        " model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(n_timesteps,n_features)))\n",
        " model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
        " model.add(Dropout(0.5))\n",
        " model.add(MaxPooling1D(pool_size=2))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(100, activation='relu'))\n",
        " model.add(Dense(n_outputs, activation='softmax'))\n",
        " model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " # fit network\n",
        " model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
        " # evaluate model\n",
        " _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        " return accuracy\n",
        " \n",
        "# summarize scores\n",
        "def summarize_results(scores):\n",
        " print(scores)\n",
        " m, s = mean(scores), std(scores)\n",
        " print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
        " \n",
        "# run an experiment\n",
        "def run_experiment(repeats=10):\n",
        " # load data\n",
        " trainX, trainy, testX, testy = load_dataset()\n",
        " # repeat experiment\n",
        " scores = list()\n",
        " for r in range(repeats):\n",
        " score = evaluate_model(trainX, trainy, testX, testy)\n",
        " score = score * 100.0\n",
        " print('>#%d: %.3f' % (r+1, score))\n",
        " scores.append(score)\n",
        " # summarize results\n",
        " summarize_results(scores)\n",
        " \n",
        "# run the experiment\n",
        "run_experiment()"
      ],
      "metadata": {
        "id": "jW3tHJWWygKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}