{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJsLPHsjFMW4oRGu2EpRj+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'            \n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense, BatchNormalization\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import plot_confusion_matrix\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import roc_auc_score\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt"],"metadata":{"id":"X6v59SLYn8fZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('../input/income/train.csv')\n","test = pd.read_csv('../input/income/test.csv')"],"metadata":{"id":"vY4xYY9kn8cx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train['education'] = train['educational-num']\n","test['education'] = test['educational-num']\n","train = train.drop(columns=['educational-num'], axis=1)\n","test = test.drop(columns=['educational-num'], axis=1)"],"metadata":{"id":"thDJ9wScn8aO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_null, test_null = [], []\n","print('=== Train Data ===')\n","for col in train.columns:\n","    x = set(train[col])\n","    y = train[col].isnull().sum()\n","    train_null.append(col) if y != 0 else None\n","    print(f'[{col}] unique data: {len(x)}. with {y} nulls')\n","\n","print()\n","print('=== Test Data ===')\n","for col in test.columns:\n","    x = set(train[col])\n","    y = train[col].isnull().sum()\n","    test_null.append(col) if y != 0 else None\n","    print(f'[{col}] unique data: {len(x)}. with {y} nulls')\n","    \n","print()\n","print(f'Train data with null: {train_null}')\n","print(f'Test data with null: {test_null}')"],"metadata":{"id":"bzLnUPegn8XW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train=train.dropna()\n","test=test.dropna()\n","\n","def dictionarize(data):\n","    temp = set(data)\n","    return { j:i+1 for i,j in enumerate(temp)}\n","\n","columns_to_classify = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'gender', 'native-country']\n","\n","for col in columns_to_classify:\n","    temp_dict = dictionarize(train[col])\n","    train[col] = [temp_dict[i] for i in train[col]]\n","    test[col] = [temp_dict[i] for i in test[col]]\n","\n","train.rename(columns={'income_>50K':'target'}, inplace=True)\n","\n","train.head()"],"metadata":{"id":"2D-JsdUAn8Ul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = train.drop(columns=['target'], axis=1)\n","y = train['target']\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2)\n","x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=.125)\n","\n","scaler = StandardScaler()\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)\n","x_val = scaler.transform(x_val)"],"metadata":{"id":"SF6x0b4zn8R0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_train.shape)\n","print(x_val.shape, y_val.shape)"],"metadata":{"id":"VjtDsdA5n8Ow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["base_model = Sequential()\n","base_model.add(Dense(13, input_dim=13, activation='relu'))\n","base_model.add(Dense(2, activation='relu'))\n","base_model.add(Dense(1, activation='sigmoid'))\n","base_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","base_model.summary()\n","\n","\n","cust_model = Sequential()\n","cust_model.add(Dense(13, kernel_initializer='he_uniform', input_dim=13, activation='relu'))\n","cust_model.add(BatchNormalization())\n","cust_model.add(Dense(32, kernel_initializer='he_uniform', activation='relu'))\n","cust_model.add(BatchNormalization())\n","cust_model.add(Dense(16, kernel_initializer='he_uniform', activation='relu'))\n","cust_model.add(Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid'))\n","cust_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","cust_model.summary()\n","\n","base_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=10, epochs=20)\n","print('='*50)\n","cust_model.fit(x_train, y_train, validation_data=(x_val, y_val), batch_size=10, epochs=20)"],"metadata":{"id":"t4g8fIrKoM2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss = base_model.history.history['loss']\n","valid_loss = base_model.history.history['val_loss']\n","train_acc = base_model.history.history['accuracy']\n","valid_acc = base_model.history.history['val_accuracy']\n","epochs = len(train_loss)\n","\n","plt.plot(range(1,epochs+1), train_loss, label='train')\n","plt.plot(range(1,epochs+1), valid_loss, label='validation')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.title('Train and Validation Loss Plot (Base Model)')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(range(1,epochs+1), train_acc, label='train')\n","plt.plot(range(1,epochs+1), valid_acc, label='validation')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.title('Train and Validation Accuracy Plot (Base Model)')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"eVF5wtSHoQZK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loss = cust_model.history.history['loss']\n","valid_loss = cust_model.history.history['val_loss']\n","train_acc = cust_model.history.history['accuracy']\n","valid_acc = cust_model.history.history['val_accuracy']\n","epochs = len(train_loss)\n","\n","plt.plot(range(1,epochs+1), train_loss, label='train')\n","plt.plot(range(1,epochs+1), valid_loss, label='validation')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.title('Train and Validation Loss Plot (Custom Model)')\n","plt.legend()\n","plt.show()\n","\n","plt.plot(range(1,epochs+1), train_acc, label='train')\n","plt.plot(range(1,epochs+1), valid_acc, label='validation')\n","plt.xlabel('epochs')\n","plt.ylabel('loss')\n","plt.title('Train and Validation Accuracy Plot (Custom Model)')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"UhtC1YwKoTRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = np.round(base_model.predict(x_test)).flatten().astype(int)\n","\n","print(\"=========================================================\\n\")\n","print(\"                    Base Model Result\")\n","print(\"=========================================================\\n\")\n","\n","acc = accuracy_score(y_test, prediction)\n","print(\"=========================================================\\n\")\n","print(\"Predicted Class (20 Samples):\")\n","print(prediction[:20])\n","print(\"\\nGround Truth (20 Samples):\")\n","print(y_test.values[:20])\n","\n","print(\"\\n=========================================================\\n\")\n","accuracy = accuracy_score(y_test, prediction)\n","accuracy = accuracy*100\n","print(f\"Accuracy: {accuracy}%\")\n","\n","f1 = f1_score(y_test, prediction, average='macro')\n","print(f\"F1 Score: {f1}\")\n","\n","auc = roc_auc_score(y_test, prediction, average='macro')\n","print(f\"AUC Score: {auc}\")\n","\n","print('\\n\\nClassification Report:')\n","cr = classification_report(y_test, prediction)\n","print(cr)"],"metadata":{"id":"BKAmZkg2oVpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prediction = np.round(cust_model.predict(x_test)).flatten().astype(int)\n","\n","print(\"=========================================================\\n\")\n","print(\"                 Custom Model Result\")\n","print(\"=========================================================\\n\")\n","\n","acc = accuracy_score(y_test, prediction)\n","print(\"=========================================================\\n\")\n","print(\"Predicted Class (20 Samples):\")\n","print(prediction[:20])\n","print(\"\\nGround Truth (20 Samples):\")\n","print(y_test.values[:20])\n","\n","print(\"\\n=========================================================\\n\")\n","accuracy = accuracy_score(y_test, prediction)\n","accuracy = accuracy*100\n","print(f\"Accuracy: {accuracy}%\")\n","\n","f1 = f1_score(y_test, prediction, average='macro')\n","print(f\"F1 Score: {f1}\")\n","\n","auc = roc_auc_score(y_test, prediction, average='macro')\n","print(f\"AUC Score: {auc}\")\n","\n","print('\\n\\nClassification Report:')\n","cr = classification_report(y_test, prediction)\n","print(cr)"],"metadata":{"id":"p_li9HQqoa1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp = pd.DataFrame(confusion_matrix(y_test, prediction), index = ['positive', 'negative'], columns = ['true', 'false'])\n","sns.heatmap(tmp, annot=True, fmt='g')"],"metadata":{"id":"xOUVEt8SodD_"},"execution_count":null,"outputs":[]}]}